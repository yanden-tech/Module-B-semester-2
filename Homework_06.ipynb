{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanden-tech/Module-B-semester-2/blob/main/Homework_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMxG_7ZN2Kbl"
      },
      "source": [
        "## Homework 06: Decision Trees\n",
        "\n",
        "So far in this module, we have built a solid foundation in machine learning by focusing on linear regression and addressing a key challenge—the **model selection problem.** We have learned how to choose the right model, measure its performance, and fine-tune its parameters so that the model generalizes well to new data.\n",
        "\n",
        "This week, we turn our attention to a fundamentally different model: **decision trees.** Unlike regression models, where we typically adjust at most one key parameter (for example, the degree of a polynomial), decision trees have many interacting parameters that can significantly affect performance. As we move from simpler models to more complex ones, it becomes essential to develop a **systematic workflow** for exploring the parameter space and optimizing model performance.\n",
        "\n",
        "Next week, we will build on this foundation by studying **ensemble methods,** where multiple trees work together to produce even stronger predictive models. These methods will incorporate the parameters we explore this week, along with additional ones, offering even more flexibility in tuning but raising even more challenges in your workflow.\n",
        "\n",
        "### What We Will Do in This Homework\n",
        "\n",
        "To identify the best set of parameters for a model with many adjustable settings, there are two main approaches. You can either iterate through the parameters **manually** or use **automated search tools** such as grid search or random search. However, both methods have drawbacks. Manual iteration can be (human) time-consuming, while automated searches can provide little insight and may not cover the most promising ranges and are (machine) time-consuming. Exhaustively searching a huge parameter space is often unrealistic.  \n",
        "\n",
        "Rather than searching the entire parameter space, we aim to **narrow down** our grid search to ranges where the optimal model is most likely to be found. The goal is to **eliminate unlikely parameter values** while still  exploring regions of uncertainty.   \n",
        "\n",
        "Plotting the MAE while only changing one of the parameters can yield important insights. Consider these plots:\n",
        "\n",
        "![Screenshot 2025-06-13 at 1.42.41 PM.png](attachment:1c79f841-7e6b-4304-9dbc-fe6f30ab405b.png)\n",
        "\n",
        "- In the first plot, there is a clear performance plateau, so searching outside the highlighted range is likely unnecessary.\n",
        "- In the second plot, the best region is less obvious, so we should not eliminate any part of the space from consideration.\n",
        "- In the third plot, clearly above a certain point, all values are equally good, so `None` (no restrictions on size) is probably the best choice.\n",
        "\n",
        "\n",
        "For these reasons, we will employ a two-phase strategy:\n",
        "\n",
        "- **First Phase:** Visualize the effect of the most important parameters by plotting and calculating the minimal CV MAE scores. Adjust the range of the plots to understand how each parameter behaves on its own or in combination with closely related parameters. This phase provides valuable insights, especially in identifying regions where performance is unstable or sensitive to parameter changes.\n",
        "- **Second Phase:** Focus on the most unstable parameter ranges identified in the first phase and perform an exhaustive search within these ranges to find the optimal model configuration.\n",
        "\n",
        "The homework is divided into four problems. The first three problems cover the first phase, and the fourth problem addresses the second phase:\n",
        "\n",
        "- **Problem One:** Tune the size and shape of the decision tree by adjusting `max_depth` and `max_leaf_nodes`.\n",
        "- **Problem Two:** Explore how the `max_features` parameter can improve performance by effectively performing on-the-fly feature selection during node splits.\n",
        "- **Problem Three:** Examine how `min_samples_split` influences performance by modifying the criteria for splitting nodes.\n",
        "- **Problem Four:** Based on the insights gained, use `GridSearchCV` to zero in on the best parameter choices.\n",
        "\n",
        "Before starting this homework, please review the lesson materials, watch the video, and download the decision tree notebook. This notebook contains code that you will refactor for this assignment—**refactoring existing code is an essential skill in machine learning.**\n",
        "\n",
        "**Note: A Change from Our Previous Coding Examples:**\n",
        "- We will use repeated CV scoring with 5 repetitions and 5 folds.\n",
        "- We will use the **Mean Absolute Error (MAE)** instead of the Root Mean Square Error (RMSE) as our CV and testing metric. Both use the original units (dollars) but MAE is less sensitive to outliers. See Appendix One for a more detailed comparison.\n",
        "\n",
        "### Grading\n",
        "\n",
        "This homework has nine (9) graded problems, each worth 5 points, and you get 5 points free if you complete the homework.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_bFZv7c2Kbn"
      },
      "outputs": [],
      "source": [
        "# Useful imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n",
        "import os\n",
        "import io\n",
        "import requests\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold,GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.tree            import DecisionTreeRegressor\n",
        "from sklearn.metrics         import mean_absolute_error\n",
        "from tqdm                    import tqdm\n",
        "\n",
        "from scipy.stats import randint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker           # Optional: you can print out y axis labels as dollars.\n",
        "\n",
        "# globals\n",
        "\n",
        "random_seed = 42\n",
        "\n",
        "# utility code\n",
        "\n",
        "# Completely optional:  Format y-axis labels as dollars with commas\n",
        "def dollar_format(x, pos):\n",
        "    return f'${x:,.0f}'\n",
        "\n",
        "def format_time(seconds):\n",
        "\n",
        "    # Convert seconds to hours, minutes, and remaining seconds\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    remaining_seconds = seconds % 60\n",
        "\n",
        "    # Return a formatted string\n",
        "    if hours == 0 and minutes == 0:\n",
        "        return f\"{seconds:.2f}s\"\n",
        "    elif hours == 0:\n",
        "        return f\"{minutes}m {remaining_seconds:.2f}s\"\n",
        "\n",
        "    return f\"{hours}h {minutes}m {remaining_seconds:.2f}s\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ2Tqo-Z2Kbo"
      },
      "source": [
        "### Load the Ames Housing Dataset  \n",
        "\n",
        "The code cell below will load the dataset for you.  This is the same dataset we used for Homework 05.\n",
        "\n",
        "> **Notice** that this code includes a useful optimization: **before downloading, it first\n",
        "checks whether the files already exist.** This is a essential step when working with large datasets or when building deep learning models, where training can span hours or even days. By reusing previously downloaded files or saved models, you can avoid unnecessary work and significantly speed up your workflow.\n",
        "\n",
        "For a detailed description of the dataset features, please refer to the **Appendix** in Homework 05."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek19Z7TG2Kbo",
        "outputId": "b0ceea14-a101-4f8d-e086-5a25a89edd2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset files already exist. Skipping download.\n",
            "Training and testing datasets loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"Ames_Dataset\"                              # Directory where files will be stored\n",
        "\n",
        "# Check if one of the files exists; if not, download and extract the zip file\n",
        "\n",
        "if not os.path.exists( os.path.join(data_dir, \"X_train.csv\") ):\n",
        "    print(\"Dataset files not found. Downloading...\")\n",
        "    zip_url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/ames_housing.zip\"\n",
        "    try:\n",
        "        response = requests.get(zip_url)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "        # Extract the zip file into the designated directory\n",
        "        with zipfile.ZipFile(io.BytesIO(response.content)) as zipf:\n",
        "            zipf.extractall(data_dir)\n",
        "        print(\"Files downloaded and extracted successfully.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading the file: {e}\")\n",
        "else:\n",
        "    print(\"Dataset files already exist. Skipping download.\")\n",
        "\n",
        "# Load the datasets\n",
        "X_train = pd.read_csv(os.path.join(data_dir, \"X_train.csv\"))\n",
        "X_test  = pd.read_csv(os.path.join(data_dir, \"X_test.csv\"))\n",
        "y_train = pd.read_csv(os.path.join(data_dir, \"y_train.csv\")).squeeze(\"columns\")\n",
        "y_test  = pd.read_csv(os.path.join(data_dir, \"y_test.csv\")).squeeze(\"columns\")\n",
        "\n",
        "print(\"Training and testing datasets loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXUFP7hR2Kbo"
      },
      "source": [
        "## Prelude: Wrapper Function for DecisionTreeRegressor\n",
        "\n",
        "In the next cell, you'll find the code from the video notebook that wraps together the model, CV scoring, and tree visualization (though visualization is not used in this homework). We made a few minor modifications:\n",
        "\n",
        "- Expanded the list of parameters to include the full set with default values.\n",
        "- Employed `RepeatedKFold` to perform 5-fold CV repeated 5 times, yielding 25 MAE scores for averaging instead of just 5.\n",
        "- Set the default value for `random_state` to `random_seed`.\n",
        "- Set the default value for the `visualize` parameter to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90YI4f3f2Kbp"
      },
      "outputs": [],
      "source": [
        "# Already provided: the code for run_decision_tree_regressor from the video notebook for week 6 with changes as noted\n",
        "\n",
        "repeated_cv = RepeatedKFold(n_repeats= 5,random_state=random_seed)    # This will perform 5-fold CV 5 times and return 25 floats instead of 5 as before\n",
        "\n",
        "# Wrapper around DecisionTreeRegressor with repeated cross-validation and using MAE\n",
        "def run_decision_tree_regressor(X_train,\n",
        "                                y_train,\n",
        "                                criterion                = 'absolute_error'  ,  # Default parameters for DecisionTreeRegressor\n",
        "                                splitter                 = 'best',\n",
        "                                max_depth                = None,\n",
        "                                min_samples_split        = 2,\n",
        "                                min_samples_leaf         = 1,\n",
        "                                min_weight_fraction_leaf = 0.0,\n",
        "                                max_features             = None,\n",
        "                                random_state             = random_seed,               # Not the default, but we'll use it consistently for reproducibility\n",
        "                                max_leaf_nodes           = None,\n",
        "                                min_impurity_decrease    = 0.0,\n",
        "                                ccp_alpha                = 0.0,\n",
        "                                n_jobs                   = -1,               # Additional parameters for CV and display of plot and results\n",
        "                                visualize                = False\n",
        "                               ):\n",
        "\n",
        "    # Initialize the DecisionTreeRegressor\n",
        "    decision_tree_model = DecisionTreeRegressor(criterion                = criterion,\n",
        "                                                splitter                 = splitter,\n",
        "                                                max_depth                = max_depth,\n",
        "                                                min_samples_split        = min_samples_split,\n",
        "                                                min_samples_leaf         = min_samples_leaf,\n",
        "                                                min_weight_fraction_leaf = min_weight_fraction_leaf,\n",
        "                                                max_features             = max_features,\n",
        "                                                random_state             = random_seed,\n",
        "                                                max_leaf_nodes           = max_leaf_nodes,\n",
        "                                                min_impurity_decrease    = min_impurity_decrease,\n",
        "                                                ccp_alpha                = ccp_alpha\n",
        "                                               )\n",
        "\n",
        "\n",
        "    # Perform cross-validation and return mean CV MAE\n",
        "    neg_MAE_scores = cross_val_score(decision_tree_model,\n",
        "                                     X_train, y_train,\n",
        "                                     scoring='neg_mean_absolute_error',\n",
        "                                     cv=repeated_cv,\n",
        "                                     n_jobs=n_jobs)\n",
        "\n",
        "    mean_cv_MAE = -np.mean(neg_MAE_scores)  # Convert negative MAE back to positive\n",
        "\n",
        "    # Train the model on the full training set for visualization purposes\n",
        "    if visualize:\n",
        "        decision_tree_model.fit(X_train, y_train)  # Train on full training data for visualization\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plot_tree(decision_tree_model, feature_names=feature_names, filled=True, rounded=True, precision=4)\n",
        "        plt.title(f\"Decision Tree Structure (max_depth={max_depth})\")\n",
        "        plt.show()\n",
        "\n",
        "    return mean_cv_MAE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOCH6XTp2Kbp"
      },
      "source": [
        "## Problem One: Controlling the Decision Tree's Shape with `max_depth` and `max_leaf_nodes`\n",
        "\n",
        "In this problem, you will explore how the structure of a decision tree impacts its performance, using the cross-validated MAE as the evaluation metric. Answer the following questions:\n",
        "\n",
        "**A.** How does the baseline model (with default parameters, except for `random_state=random_seed`) perform on the dataset?\n",
        "\n",
        "**B.** When adjusting only the `max_depth` parameter, which depth produces the lowest CV MAE?\n",
        "\n",
        "**C.** When adjusting only the `max_leaf_nodes` parameter, which value results in the best performance?\n",
        "\n",
        "**D.** Can a combination of `max_depth` and `max_leaf_nodes` yield a lower CV MAE than optimizing either parameter independently?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WzL5-2d2Kbp"
      },
      "source": [
        "### Part 1.A\n",
        "\n",
        "In the next cell, use `run_decision_tree_regressor` to establish a baseline CV MAE score for the **training set.** Keep all the parameters at their default, but set `random_state=random_seed`.\n",
        "\n",
        "Note: The `run_decision_tree_regressor` returns the MAE of the repeated CV score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QDx_Uk52Kbp",
        "outputId": "0d27641a-3fe3-4ff8-a433-4534f5072000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline CV MAE: 25472.84\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "baseline_cv_mae=run_decision_tree_regressor(X_train,y_train,random_state=random_seed)\n",
        "\n",
        "print(f\"Baseline CV MAE: {baseline_cv_mae:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Pw9jD82Kbp"
      },
      "source": [
        "### Problem 1.A Graded Answer\n",
        "\n",
        "Set `a1a` to the baseline CV MAE score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMfJj1Io2Kbq"
      },
      "outputs": [],
      "source": [
        " # Your answer here; use an expression, not a constant derived by examining the data\n",
        "\n",
        "a1a =   run_decision_tree_regressor(X_train,y_train,random_state=random_seed)                   # replace 0 with an expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gHwN6IZ2Kbq",
        "outputId": "f619d377-1797-4f69-cb54-34d89071ded7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a1a = $25,472.84\n"
          ]
        }
      ],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a1a = ${a1a:,.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDI7k9uf2Kbq"
      },
      "source": [
        "### Part 1.B\n",
        "\n",
        "In the next cell, adapt the code from the video notebook to plot CV MAE vs max_depth with the following modifications:\n",
        "\n",
        "- Determine an appropriate range of `max_depth` values that zero in on the optimum. If the best value occurs at either end of your range, it indicates you may not have captured the true minimum—expand your range accordingly.\n",
        "- Update your plot labels and printout to reflect that you are reporting the mean absolute error (MAE) in dollars. Use the following line in your plotting code to format the Y-axis labels in dollars:\n",
        "  ```python\n",
        "  plt.gca().yaxis.set_major_formatter(mticker.FuncFormatter(dollar_format))\n",
        "  ```\n",
        "\n",
        "**Note:** The code in the video notebook may not always use default settings. For this experiment, ensure that all parameters except `max_depth` are set to their default values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cut and paste and modify the code from the video notebook as needed to plot and find the best max_depth\n",
        "\n",
        "\n",
        "# Dollar formatting function\n",
        "def dollar_format(x, pos):\n",
        "    return f'${x:,.0f}'\n",
        "\n",
        "max_depth_values = range(1, 20)  # Example initial range\n",
        "cv_mae_scores = []\n",
        "\n",
        "# Run decision tree regressor for each max_depth\n",
        "for depth in max_depth_values:\n",
        "    mae = run_decision_tree_regressor(X_train, y_train, max_depth=depth)\n",
        "    cv_mae_scores.append(mae)\n",
        "    print(f\"max_depth={depth}, CV MAE: ${mae:,.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLW4Lu7bcq52",
        "outputId": "12decadc-3772-41d2-9c55-3cde37eb774c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth=1, CV MAE: $40,030.94\n",
            "max_depth=2, CV MAE: $33,898.72\n",
            "max_depth=3, CV MAE: $29,622.12\n",
            "max_depth=4, CV MAE: $26,241.81\n",
            "max_depth=5, CV MAE: $24,686.46\n",
            "max_depth=6, CV MAE: $23,812.19\n",
            "max_depth=7, CV MAE: $23,564.53\n",
            "max_depth=8, CV MAE: $23,794.92\n",
            "max_depth=9, CV MAE: $23,990.92\n",
            "max_depth=10, CV MAE: $24,382.19\n",
            "max_depth=11, CV MAE: $24,582.18\n",
            "max_depth=12, CV MAE: $24,702.43\n",
            "max_depth=13, CV MAE: $24,686.28\n",
            "max_depth=14, CV MAE: $25,194.22\n",
            "max_depth=15, CV MAE: $25,055.54\n",
            "max_depth=16, CV MAE: $25,292.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyX83zeY2Kbq",
        "outputId": "38f71611-ab1b-408a-8919-71b13c538d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth=15, CV MAE: $25,055.54\n",
            "max_depth=16, CV MAE: $25,292.52\n",
            "max_depth=17, CV MAE: $25,192.51\n",
            "max_depth=18, CV MAE: $25,146.59\n",
            "max_depth=19, CV MAE: $25,466.07\n",
            "max_depth=20, CV MAE: $25,184.70\n",
            "max_depth=21, CV MAE: $25,365.68\n",
            "max_depth=22, CV MAE: $25,214.15\n",
            "max_depth=23, CV MAE: $25,554.76\n",
            "max_depth=24, CV MAE: $25,005.46\n",
            "max_depth=25, CV MAE: $25,332.54\n",
            "max_depth=26, CV MAE: $25,392.49\n",
            "max_depth=27, CV MAE: $25,359.11\n",
            "max_depth=28, CV MAE: $25,364.19\n",
            "max_depth=29, CV MAE: $25,130.98\n",
            "max_depth=30, CV MAE: $25,421.67\n"
          ]
        }
      ],
      "source": [
        "# Cut and paste and modify the code from the video notebook as needed to plot and find the best max_depth\n",
        "\n",
        "\n",
        "# Dollar formatting function\n",
        "def dollar_format(x, pos):\n",
        "    return f'${x:,.0f}'\n",
        "\n",
        "max_depth_values = range(15, 31)  # Example initial range\n",
        "cv_mae_scores = []\n",
        "\n",
        "# Run decision tree regressor for each max_depth\n",
        "for depth in max_depth_values:\n",
        "    mae = run_decision_tree_regressor(X_train, y_train, max_depth=depth)\n",
        "    cv_mae_scores.append(mae)\n",
        "    print(f\"max_depth={depth}, CV MAE: ${mae:,.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depth_values, cv_mae_scores, marker='o', linestyle='-')\n",
        "\n",
        "plt.title('Cross-Validated MAE vs. Tree Depth')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Cross-Validated MAE (Dollars)')\n",
        "plt.grid(True)\n",
        "\n",
        "# Format Y-axis as dollars\n",
        "plt.gca().yaxis.set_major_formatter(mticker.FuncFormatter(dollar_format))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tlG_vvxxXDgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSkQfzhh2Kbq"
      },
      "source": [
        "### Problem 1.B Graded Answer\n",
        "\n",
        "Set `a1b` to the depth found which results in the lowest CV MAE score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMKtA7md2Kbr"
      },
      "outputs": [],
      "source": [
        "# Your answer here; use an expression, not a constant derived by examining the data\n",
        "\n",
        "a1b = max_depth_values[cv_mae_scores.index(min(cv_mae_scores))]                     # Replace 0 with an expression or a variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBTKx78t2Kbr"
      },
      "outputs": [],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a1b = {a1b}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHtmb0XD2Kbr"
      },
      "source": [
        "### Part 1.C\n",
        "\n",
        "Now, perform the same experiment as in Part 1.B—but this time, focus on tuning the `max_leaf_nodes` parameter.\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "- Ensure that `max_size` is set to `None` so that you can isolate the effect of `max_leaf_nodes` on limiting the size of the tree.\n",
        "- The range of values for `max_leaf_nodes` will differ considerably from that used for `max_depth`. Experiment with a wide range until you find a minimum that does not occur at either end of your tested interval.\n",
        "\n",
        "Remember to apply the same modifications as in Part 1.B, including updating your plot labels and printout to reflect that the metric is reported in dollars (using the line below to format the y-axis):\n",
        "\n",
        "```python\n",
        "plt.gca().yaxis.set_major_formatter(mticker.FuncFormatter(dollar_format))\n",
        "```\n",
        "\n",
        "After generating your plot and results, provide your answer for the graded question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGgke4ng2Kbr"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5taM_G8L2Kbr"
      },
      "source": [
        "### Problem 1.C Graded Answer\n",
        "\n",
        "Set `a1c` to the maximum number of leaf nodes that provided the best CV score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FXmPiDK2Kbr"
      },
      "outputs": [],
      "source": [
        "# Your answer here; use an expression, not a constant derived by examining the data\n",
        "\n",
        "a1c = 0                    # Replace 0 with expression or variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK9efTdO2Kbr",
        "outputId": "461f9f13-ca2e-42d9-a86b-68c1b2d4d50f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a1c = 0\n"
          ]
        }
      ],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a1c = {a1c}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQj6hBtH2Kbs"
      },
      "source": [
        "### Part 1.D\n",
        "\n",
        "Limiting the size of a decision tree using `max_depth` (depth) and `max_leaf_nodes` (width) affects the model in different ways. A natural question arises: is there a combination of these two parameters that yields a better score than tuning either one alone?\n",
        "\n",
        "\n",
        "**What to Do:**\n",
        "\n",
        "- In order to not disturb your original experiment, copy the code from Parts 1.B and 1.C as needed into new cells below, and go back and forth as needed, adjusting the ranges to see if you can \"hone in\" on the best combination of the two.\n",
        "- Think about the strategy shown in the three plots at the beginning of the homework.\n",
        "- Keep in mind the possibility that the defaults may in one or the other case be the best choice!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce1DhHOD2Kbs"
      },
      "source": [
        "### Solution:\n",
        "\n",
        "Redo 1.B (resulting in None) then see that the original result of 1.C is already optimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otgEeFXV2Kbs"
      },
      "outputs": [],
      "source": [
        "# Cut and paste the code from the video notebook to plot and find the best max_depth\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXEH7CNq2Kbs"
      },
      "source": [
        "### Problem 1.D Graded Answer   \n",
        "\n",
        "Set `a1d` to the tuple of the best max_depth and the best max_leaf_nodes parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ8kSRGj2Kbs"
      },
      "outputs": [],
      "source": [
        "# Your answer here\n",
        "\n",
        "a1d = 0,0                    # Replace 0,0 with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE5tCwjx2Kbs",
        "outputId": "e1c787f4-db15-489a-d6a5-03eaa4662832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a1d = (0, 0)\n"
          ]
        }
      ],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a1d = {a1d}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZSuH0nj2Kbs"
      },
      "source": [
        "## Problem Two: Adding Feature Selection and Randomness with max_features\n",
        "\n",
        "In this problem, you will build on the best model from Problem One by exploring the effect of the `max_features` parameter. This parameter does two key things:\n",
        "\n",
        "- It performs variable selection at the level of each split by considering only a subset of features (we will revisit this idea in Week 9).\n",
        "- It randomly chooses this subset at each split, encouraging the model to explore different parts of the feature space. This randomness can improve performance, especially when used with ensemble methods (which we will study next week).\n",
        "\n",
        "What to Do:\n",
        "\n",
        "- Use the best model from Problem One and vary only the `max_features` parameter.\n",
        "- Experiment with a range of values from 10 up to the total number of features in the dataset.\n",
        "- Print out the plot and the results, and answer the graded question as usual.\n",
        "- Do not use `GridSearchCV` for this problem.\n",
        "- Do not reset all parameters to their defaults; keep the best settings from Problem One and only change `max_features`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1R--ner2Kbs"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi-Fi-0d2Kbs"
      },
      "source": [
        "### Problem 2 Graded Answer\n",
        "\n",
        "This may or may not have improved the score.  In any case, set `a2` to the best parameter choice, remembering that `None`\n",
        "is the same as the maximum value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxP_XrK2Kbs"
      },
      "outputs": [],
      "source": [
        "# Your answer here\n",
        "\n",
        "a2 = 0                    # Replace 0 with an expression or a variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bn8vmKd2Kbs",
        "outputId": "ddd09ec2-a3a8-4ccd-ae46-94cd981bb118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a2 = 0\n"
          ]
        }
      ],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a2 = {a2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIsoEVQL2Kbt"
      },
      "source": [
        "## Problem Three: Controlling Node Splitting with min_samples_split\n",
        "\n",
        "In this problem, you will investigate how the min_samples_split parameter affects your decision tree’s performance. Using the best model configuration obtained from Problems One **and Two,** vary only the `min_samples_split` parameter while keeping all other parameters fixed.\n",
        "\n",
        "**What to Do:**\n",
        "\n",
        "- As before, experiment with different values of `min_samples_split`, starting with a lower bound of 2.\n",
        "- Plot the CV MAE against the different values of `min_samples_split` and print out your results.\n",
        "- Do not use `GridSearchCV` for this experiment.\n",
        "- Make sure that only the `min_samples_split` parameter is varied; all other parameters should remain at the best settings you’ve found so far.\n",
        "- Finally, answer the graded question based on your findings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYJ6cPkE2Kbt"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePU6tqdo2Kbt"
      },
      "source": [
        "### Problem 3 Graded Answer\n",
        "\n",
        "Again, this may or may not have found a lower CV score.\n",
        "\n",
        "Assign `a3` to the best choice for the minimum number of samples to perform a split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EmA6BNe2Kbt"
      },
      "outputs": [],
      "source": [
        "# Your answer here; use an expression, not a constant derived by examining the data\n",
        "\n",
        "a3 = 0                    # Replace 0 with an expression or a variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_gOOcm12Kbt",
        "outputId": "0cd31194-66da-46f7-f973-593ca4e2f8d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a3 = 0\n"
          ]
        }
      ],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a3 = {a3}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5iiEoxC2Kbt"
      },
      "source": [
        "## **Problem Four: Fine-Tuning Your Model with Grid Search**\n",
        "\n",
        "Now that we have a solid understanding of how our parameters influence model performance, it’s time to refine our search. Instead of testing large parameter ranges blindly, we will **use insights from previous experiments** to focus on the most promising values—balancing accuracy with computational efficiency.\n",
        "\n",
        "### **What to Do**\n",
        "- Using insights from **Problems One through Three**, select **refined parameter ranges** for the three key parameters.\n",
        "- Perform a **grid search** over these restricted ranges.\n",
        "    - Be sure to include this among the parameters to `param_grid`:\n",
        ">  `'criterion': [\"absolute_error\"]`\n",
        "    - Be sure to include repeated CV scoring in the parameter to `GridSearchCV`:\n",
        "> `cv=repeated_cv`\n",
        "- Print out the **top 10 results**.\n",
        "- Answer the graded questions.\n",
        "\n",
        "**Note: Grid search is a compute hog!**  Start with modest ranges around your best values, and see what happens. Running a large set of ranges will almost always take way too long!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QynzX5rC2Kbt"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr2gT7nR2Kbt"
      },
      "source": [
        "### Problem 4 Graded Answers\n",
        "\n",
        "Set `a4a` to a tuple of the four optimal parameter values found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ciUNzNz2Kbu"
      },
      "outputs": [],
      "source": [
        "# Your answer here\n",
        "\n",
        "a4a = 0,0,0,0          # Your answer should be a tuple ( best max_depth, best max_leaf_nodes, best max_features, best min_samples_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOHVgsyt2Kbu",
        "outputId": "64c6bdcc-5a06-4f99-ec5d-1d7d2fbfd456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a4a = (0, 0, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a4a = {a4a}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvy4HhBU2Kbu"
      },
      "source": [
        "Set `a4b` to the best MAE score found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lATxtXEZ2Kbu"
      },
      "outputs": [],
      "source": [
        "# Your answer here\n",
        "\n",
        "a4b = 0                    # Cut and paste from the gridsearch listing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik9GyKAB2Kbu",
        "outputId": "3683ae09-eb1b-435a-9c06-db32d3bd4d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a4b = $0.00\n"
          ]
        }
      ],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a4b = ${a4b:,.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1PKfMX42Kbu"
      },
      "source": [
        "### Final Exam Time!\n",
        "\n",
        "Run your best model through the test set using `run_decision_tree_regressor` and report the test score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hC_kT6o2Kbu"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRzqq1O72Kbu"
      },
      "outputs": [],
      "source": [
        "# Your answer here\n",
        "\n",
        "a4c = 0                    # Replace with the test MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCiflNWy2Kbu",
        "outputId": "0ca3f8d4-8975-4569-931e-87e21f0d0618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a4c = $0.00\n"
          ]
        }
      ],
      "source": [
        "# DO NOT change this cell in any way\n",
        "\n",
        "print(f'a4c = ${a4c:,.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw3Bj9d02Kbu"
      },
      "source": [
        "## Appendix One: Root Mean Square Error or Mean Absolute Error?\n",
        "\n",
        "### RMSE vs. MAE for a Decision-Tree Regressor on *Ames Housing*\n",
        "\n",
        "\n",
        "| Metric | How it treats errors | When it’s a good fit | Practical notes for **Ames Housing** |\n",
        "|--------|---------------------|----------------------|---------------------------------------|\n",
        "| **RMSE**  ($\\sqrt{\\text{MSE}}$) | Squares errors before averaging → **large mistakes dominate** | • Punishes big misses (e.g., overpricing a home by \\$100 k) <br>• Standard metric in ML competitions and much theoretical work | Ames has a long right tail of high-priced homes; RMAE will be driven by a handful of expensive houses and could encourage the tree to over-fit them. |\n",
        "| **MAE**  ($\\displaystyle\\text{mean}\\,|y-\\hat y|$) | Treats every \\$1 error equally → **linear penalty** | • Robust to outliers <br>• Error is expressed directly in “dollars,” which aids intuition | Provides a more stable view of model quality across the whole price range; easier to interpret (“average error ≈ \\$17 k”). |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGaP1oZL2Kbu"
      },
      "source": [
        "## Appendix Two: Which DecisionTree parameters are most important?\n",
        "\n",
        "When first exploring  a complex model such as decision trees, it’s best to focus first on those parameters that most directly affect the shape of the tree and the method used for splitting nodes. Some parameters are more important than others, especially when first using a model. Here is an approximate ordering of the parameters from most significant to least. We will only experiment with the first 4 of these parameters in this homework.\n",
        "\n",
        "---\n",
        "\n",
        "1. **max_depth** (default: **None**)  \n",
        "   *Controls the maximum depth of the tree, which is crucial for managing overfitting and overall model complexity.*\n",
        "\n",
        "2. **max_leaf_nodes** (default: **None**)  \n",
        "   *An alternative to max_depth, this limits the number of leaf nodes and can be used to control tree size in a different way.*\n",
        "\n",
        "3. **max_features** (default: **None**)  \n",
        "   *Determines the maximum number of features considered when looking for the best split. Tuning this can affect the bias-variance trade-off.*\n",
        "\n",
        "4. **min_samples_split** (default: **2**)  \n",
        "   *Specifies the minimum number of samples required to split an internal node. It influences how “greedy” the tree is in creating splits.*\n",
        "\n",
        "---\n",
        "\n",
        "5. **criterion** (default: **'squared_error'**)  \n",
        "   *Chooses the function to measure the quality of a split (e.g., squared_error, absolute_error, etc.). Experimenting with this can reveal how different error metrics impact performance.*\n",
        "\n",
        "6. **splitter** (default: **'best'**)  \n",
        "   *Decides the strategy used to choose the split at each node (typically \"best\" or \"random\"). While its effect is subtler, exploring it can be educational.*\n",
        "\n",
        "7. **min_samples_leaf** (default: **1**)  \n",
        "   *Sets the minimum number of samples that must be present in a leaf. This parameter helps prevent creating leaves with very few samples, thus reducing overfitting.*\n",
        "\n",
        "\n",
        "8. **ccp_alpha** (default: **0.0**)  \n",
        "   *The complexity parameter for Minimal Cost-Complexity Pruning. It’s useful for understanding pruning techniques, though it’s a bit more advanced.*\n",
        "\n",
        "9. **min_impurity_decrease** (default: **0.0**)  \n",
        "   *A node will be split only if the split results in a decrease in impurity greater than or equal to this threshold. It provides another way to control overfitting.*\n",
        "\n",
        "10. **min_weight_fraction_leaf** (default: **0.0**)  \n",
        "    *Ensures that each leaf has a minimum weighted fraction of the total. It’s less commonly tuned but can be relevant when sample weights matter.*\n",
        "\n",
        "11. **random_state** (default: **None**)  \n",
        "    *Used only for reproducibility, traditionally as `random_state = random_seed`. It doesn’t affect the model’s learning, but ensures that results are consistent across runs. Set to `None` for more realistic random behavior typical for deployed systems.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}